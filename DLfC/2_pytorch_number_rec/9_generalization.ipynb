{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialize the Random Parameters\n",
    "* Calculate the Predictions\n",
    "* Calculate the Loss\n",
    "* Calculate the Gradients\n",
    "* Update the Weights\n",
    "* Go to Step Two and Repeat the Process\n",
    "* Stop When the Model is Good Enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "from fastai.vision.widgets import *\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Path object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vadim\\.fastai\\data\\mnist_sample\n"
     ]
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill the arrays of path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('C:/Users/Vadim/.fastai/data/mnist_sample/train/3/10011.png'), Path('C:/Users/Vadim/.fastai/data/mnist_sample/train/3/10031.png'), Path('C:/Users/Vadim/.fastai/data/mnist_sample/train/3/10034.png'), Path('C:/Users/Vadim/.fastai/data/mnist_sample/train/3/10042.png')]\n",
      "len(threes) = 6131, len(sevens) = 6265\n"
     ]
    }
   ],
   "source": [
    "threes = (path/'train'/'3').ls().sorted()\n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "print(threes[2:6])\n",
    "print(f\"{len(threes) = }, {len(sevens) = }\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing an image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5klEQVR4nGNgoD9gRGJr+aSevsAw4Rc2demf/v379++fE1ZDhF78+/fv3793blhlM778e/Dv379e7A44/+/Sv3//lLBLhpz79+/fP00cbpe4+O/fv9VwLguyXLSeDgMDw1Fs2jSu/fr3D8VOJoSkpiLEmAKsNuZ9+/cPp52TbgswsEzmw+FYBgYGxoZ/t+VxSbL/+3dNBpdk179/JehiwhujGBgYGBgkP2AJviX/rturMBhHnvv3r5sDXdLy6L9/97Z8/Pfv71VuTLt6Mv/9+/fv3783yIIwf5aw8zAYRDJ8xB7TdAQABFdhZWAfWxoAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(threes[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,  42, 118, 219, 166, 118, 118,   6,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [103, 242, 254, 254, 254, 254, 254,  66,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 18, 232, 254, 254, 254, 254, 254, 238,  70,   0,   0,   0,   0,   0,   0],\n",
       "        [  0, 104, 244, 254, 224, 254, 254, 254, 141,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0, 207, 254, 210, 254, 254, 254,  34,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,  84, 206, 254, 254, 254, 254,  41,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,  24, 209, 254, 254, 254, 171,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,  91, 137, 253, 254, 254, 254, 112,   0,   0,   0,   0,   0,   0],\n",
       "        [ 40, 214, 250, 254, 254, 254, 254, 254,  34,   0,   0,   0,   0,   0,   0],\n",
       "        [ 81, 247, 254, 254, 254, 254, 254, 254, 146,   0,   0,   0,   0,   0,   0],\n",
       "        [  0, 110, 246, 254, 254, 254, 254, 254, 171,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,  73,  89,  89,  93, 240, 254, 171,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   1, 128, 254, 219,  31,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   7, 254, 254, 214,  28,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 138, 254, 254, 116,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  25, 240, 254, 254,  34,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(Image.open(threes[0]))\n",
    "_[3:20, 10:25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
    "len(three_tensors), len(seven_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tensor of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten1 = torch.tensor([1, 2, 3])\n",
    "ten2 = torch.tensor([7, 8, 9])\n",
    "torch.stack([ten1, ten2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6131 tensors 28*28, total: 6131 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6131, 28, 28])\n",
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.1137, 0.5882, 0.7647, 0.9961, 1.0000, 0.9961, 0.6902, 0.7569],\n",
      "         [0.1882, 0.6510, 0.8784, 0.9922, 0.9922, 0.9176, 0.7686, 0.9922, 0.9922, 0.9922],\n",
      "         [0.9765, 0.9922, 0.7333, 0.1804, 0.0392, 0.0314, 0.0157, 0.0392, 0.7608, 0.9922],\n",
      "         [0.9922, 0.9020, 0.1882, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7529, 0.9922],\n",
      "         [0.0784, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1686, 0.8784, 0.9922],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9765, 0.9922, 0.9608],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0549, 0.3961, 0.8745, 0.9922, 0.9725, 0.4863],\n",
      "         [0.0000, 0.0000, 0.0431, 0.6510, 0.9373, 0.9922, 0.9922, 0.9922, 0.7333, 0.1176],\n",
      "         [0.0000, 0.0000, 0.0627, 0.9725, 0.9804, 0.9922, 0.9922, 0.9922, 0.9922, 0.9098],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.1686, 0.3843, 0.3843, 0.8157, 0.9922, 0.9922]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0039, 0.0706, 0.3647, 0.6431, 0.9961, 1.0000, 0.9961],\n",
      "         [0.0000, 0.0510, 0.1412, 0.5922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922],\n",
      "         [0.0000, 0.3490, 0.9922, 0.9922, 0.9922, 0.9608, 0.6745, 0.3216, 0.3216, 0.3216],\n",
      "         [0.0000, 0.3490, 0.9922, 0.9882, 0.6706, 0.2118, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0667, 0.5922, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3255, 0.8078, 0.9294],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2745, 0.5608, 0.9529, 0.9843, 0.9922, 0.9922],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0157, 0.8000, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0196, 0.9216, 0.9922, 0.9922, 0.8667, 0.6549, 0.3686],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.4314, 0.6863, 0.2980, 0.0706, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
    "stacked_threes = torch.stack(three_tensors).float()/255\n",
    "print(stacked_threes.shape)\n",
    "# 2 image: first and second, 15 * 10\n",
    "print(stacked_threes[1:3, :15, 7:17])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate two tensors of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [7, 8, 9],\n",
      "        [1, 2, 3],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "tensor_1_1 = torch.tensor([1, 2, 3])\n",
    "tensor_1_2 = torch.tensor([4, 5, 6])\n",
    "tensor_1 = torch.stack([ten1, ten2])\n",
    "\n",
    "tensor_2_1 = torch.tensor([90, 80, 70])\n",
    "tensor_2_2 = torch.tensor([60, 50, 40])\n",
    "tensor_2 = torch.stack([ten1, ten2])\n",
    "\n",
    "tensor_3 = torch.cat([tensor_1, tensor_2])\n",
    "print(tensor_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "view is change tensor shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12396, 784])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28 * 28)\n",
    "train_x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tensor of labels for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n",
      "tensor([[1, 2, 3, 4]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(x)\n",
    "print(torch.unsqueeze(x, 0))\n",
    "print(torch.unsqueeze(x, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        ...,\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "train_y = tensor([1] * len(threes) + [0] * len(sevens)).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incoporete data and targets to dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of tensors ```[[data1, answer1], ...]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = list(zip(train_x, train_y))\n",
    "x, y = dset[0]\n",
    "x.shape, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataLoader object with batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fastai.data.load.DataLoader object at 0x0000025332481480>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb, yb = first(dl)\n",
    "xb.shape, yb.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same with the valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1010, 28, 28]) torch.Size([1028, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "valid_3_tens = torch.stack([tensor(Image.open(o))\n",
    "                           for o in (path/'valid'/'3').ls()])\n",
    "valid_3_tens = valid_3_tens.float()/255\n",
    "\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o))\n",
    "                           for o in (path/'valid'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255\n",
    "\n",
    "print(valid_3_tens.shape, valid_7_tens.shape)\n",
    "\n",
    "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28 * 28)\n",
    "valid_y = tensor([1] * len(valid_3_tens) + [0] *\n",
    "                 len(valid_7_tens)).unsqueeze(1)\n",
    "\n",
    "valid_dset = list(zip(valid_x, valid_y))\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "where(condition, case_true, case_false)\n",
    "if condition: case_true\n",
    "else: case_false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "func mnist_loss always return loss between [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets == 1, 1 - predictions, predictions).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model -- is a function that calculates predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds > 0.5) == yb\n",
    "    return correct.float().mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Linear does the same thing as our init_params and linear together. It\n",
    "contains both the weights and biases in a single class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every PyTorch module knows what parameters it has that can be trained; they are\n",
    "available through the parameters method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = linear_model.parameters()\n",
    "w.shape, b.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create own optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create BasicOptim objects with defined parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model):\n",
    "    for x_batch, y_batch in dl:\n",
    "        calc_grad(x_batch, y_batch, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4705"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_epoch(linear_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One train loop (20 epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs):\n",
    "    for _ in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 "
     ]
    }
   ],
   "source": [
    "train_model(linear_model, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creater optimiser using fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 0.9535 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28, 1)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model(linear_model, 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai also provides Learner.fit, which we can use instead of train_model.\n",
    "\n",
    "To create a Learner, we first need to create a DataLoaders, by passing in our\n",
    "training and validation DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(dl, valid_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls,\n",
    "                nn.Linear(28*28, 1),\n",
    "                opt_func=SGD,\n",
    "                loss_func=mnist_loss,\n",
    "                metrics=batch_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-83.022614</td>\n",
       "      <td>-335.850677</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-309.395782</td>\n",
       "      <td>-672.167480</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-570.783752</td>\n",
       "      <td>-1008.484375</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-855.037354</td>\n",
       "      <td>-1344.801270</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-1152.467773</td>\n",
       "      <td>-1681.118042</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-1456.801025</td>\n",
       "      <td>-2017.434937</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-1764.505493</td>\n",
       "      <td>-2353.751953</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-2073.773682</td>\n",
       "      <td>-2690.068604</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-2383.741943</td>\n",
       "      <td>-3026.385742</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-2694.014160</td>\n",
       "      <td>-3362.702148</td>\n",
       "      <td>0.953386</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(n_epoch=10, lr=lr)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ab5ab083b7b49248b316ea110ecb19df829c49251bfed4a58e3f55a0efabe42"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
